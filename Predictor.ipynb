{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import requests\n",
    "import json\n",
    "import sys, os\n",
    "import itertools\n",
    "import time\n",
    "import numpy as np\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "from binance.spot import Spot\n",
    "from twelvedata import TDClient\n",
    "from sklearn import metrics\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "import itertools\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "from typing import Union, Dict, List\n",
    "import talib\n",
    "import ta\n",
    "import shap\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"..\"))\n",
    "import fin_utilities \n",
    "cfg = fin_utilities.__cfg_reading(\"pred\")\n",
    "import my_functions\n",
    "\n",
    "import matplotlib as mpl\n",
    "# Set the default color cycle\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "\n",
    "\n",
    "#td = TDClient(apikey=cfg['TWELVEDATA']['API'])  \n",
    "\n",
    "PROJECT_DIR = eval(cfg['PROJECT_PATH'])\n",
    "DATA_PATH   = PROJECT_DIR / cfg['DATA_FOLDER']\n",
    "SOURCE      = cfg['API_DATA_SOURCE']\n",
    "URL         = cfg[SOURCE]['API_URL_HIST_DATA']\n",
    "COLUMNS     = cfg[SOURCE]['COLUMN_NAMES']\n",
    "PARAMS      = cfg[SOURCE]['REQ_PARAMS']\n",
    "PAIR        = cfg['SYMBOL'] + cfg['STABLECOIN']\n",
    "MAX_LENGTH  = cfg['MAX_TRADE_DURATION']\n",
    "XGB_PARAM   = cfg['xgb']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8780c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SOURCE == 'BINANCE':\n",
    "    PARAMS.update({'startTime': my_functions.datetime_to_ts(PARAMS['startTime'])})\n",
    "    PARAMS.update({'endTime': my_functions.datetime_to_ts(PARAMS['endTime'])})\n",
    "\n",
    "PARAMS  \n",
    "\n",
    "limit=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed463cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_other_symbols = True\n",
    "# new_connection\n",
    "client = Spot()\n",
    "\n",
    "df = my_functions.load_data(client, PAIR, DATA_PATH, PARAMS, COLUMNS)\n",
    "\n",
    "\n",
    "if compare_other_symbols:\n",
    "\n",
    "    pairs_available = my_functions.get_all_pairs_available(client)\n",
    "\n",
    "    # prendo le top coin e rimuovo il simbolo in cfg se c'Ã¨\n",
    "    symbols, actualized_mc = my_functions.get_top_symbols_by_marketcap(50, cfg['STABLECOIN'])\n",
    "    symbols = [el for el in symbols if el not in PAIR]\n",
    "\n",
    "    for s in symbols:\n",
    "\n",
    "        if s in pairs_available:\n",
    "            \n",
    "            df = pd.concat([df,my_functions.load_data(client, s, DATA_PATH, PARAMS, COLUMNS)], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d502484",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['OpenTimestamp', 'Ignore.'], inplace=True, errors='ignore')\n",
    "df['Mkt_Cap'] = df['Close']*df['Volume']\n",
    "\n",
    "df_mkt_cap = df.groupby(['CloseTimestamp'])['Mkt_Cap'].sum().reset_index()\n",
    "df_mkt_cap.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00192c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_btc = pd.merge(df[df['Symbol']==PAIR], df_mkt_cap, on='CloseTimestamp', suffixes=[None, '_tot'])\n",
    "df_btc.drop(columns=['Symbol', 'Quote asset volume','Taker buy base asset volume','Taker buy quote asset volume'], inplace=True)\n",
    "df_btc['CloseTimestamp'] = df_btc['CloseTimestamp'].dt.date\n",
    "df_btc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf95515",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_btc['Norm_wgt_vol'] = df['Volume']*(df_btc['Open']+df_btc['High']+df_btc['Low']+df_btc['Close'])/(4*df_btc['Number of trades'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461aa122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "\n",
    "# get levels using the first method\n",
    "levels_1 = my_functions.detect_level_method_1(df_btc)\n",
    "# get levels using the second method\n",
    "levels_2 = my_functions.detect_level_method_2(df_btc)\n",
    "\n",
    "\n",
    "\n",
    "def plot_candlestick_aline(df_, width, height, support,resistance):\n",
    "    \"\"\"\n",
    "    Plot candlestick chart of the dataframe\n",
    "    \"\"\"\n",
    "    dict(hlines=[support,resistance],colors=['g','r'],linestyle='-.')\n",
    "    df = df_.copy()\n",
    "    df.rename(columns={df.columns[0]:'Date'}, inplace=True)\n",
    "    df[df.columns[0]] = pd.to_datetime(df[df.columns[0]])\n",
    "    df=df.set_index(df.columns[0])\n",
    "    mpf.plot(df,type='candle', style='charles', figsize=(width, height),alines=dict(alines=support,linestyle='-.'))\n",
    "\n",
    "def plot_candlestick_hline(df_, width, height, supports,resistance):\n",
    "    \"\"\"\n",
    "    Plot candlestick chart of the dataframe\n",
    "    \"\"\"\n",
    "    df = df_.copy()\n",
    "    df.rename(columns={df.columns[0]:'Date'}, inplace=True)\n",
    "    df[df.columns[0]] = pd.to_datetime(df[df.columns[0]])\n",
    "    df=df.set_index(df.columns[0])\n",
    "    mpf.plot(df,type='candle', style='charles', figsize=(width, height), hlines=list(supports))\n",
    "\n",
    "def determine_optimal_clusters(data, max_clusters, plot=False):\n",
    "    \"\"\"\n",
    "    Determines the optimal number of clusters for a given dataset using the elbow method.\n",
    "    \"\"\"\n",
    "\n",
    "    wcss = []\n",
    "    silhouette_scores = []\n",
    "\n",
    "    # Loop through a range of possible number of clusters\n",
    "    for i in range(2, max_clusters+1):\n",
    "        kmeans = KMeans(n_clusters=i, init='k-means++', n_init=5, random_state=42)\n",
    "        labels = kmeans.fit_predict(data)\n",
    "        silhouette_scores.append(silhouette_score(data, labels))\n",
    "        kmeans.fit(data)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "\n",
    "    # Find the \"elbow\" point on the plot\n",
    "    kneed_clusters = None\n",
    "    for i in range(2, len(wcss)):\n",
    "        if wcss[i] - wcss[i-1] < wcss[i-1] - wcss[i-2]:\n",
    "            kneed_clusters = i\n",
    "            break\n",
    "\n",
    "    optimal_clusters = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "    \n",
    "    if plot:\n",
    "        # Plot the within-cluster variation for each number of clusters\n",
    "        plt.plot(range(2, max_clusters+1), wcss)\n",
    "        plt.title('Elbow Method')\n",
    "        plt.xlabel('Number of clusters')\n",
    "        plt.ylabel('Within-cluster variation')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(range(2, max_clusters+1), silhouette_scores)\n",
    "        plt.title('Silhouette Method')\n",
    "        plt.xlabel('Number of clusters')\n",
    "        plt.ylabel('Silhouette Score')\n",
    "        plt.show()\n",
    "\n",
    "    return kneed_clusters, optimal_clusters\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# determine the optimal number of clusters\n",
    "kneed, silhouette = determine_optimal_clusters(df_btc[['High','Low','Norm_wgt_vol']],20, False)\n",
    "print(kneed, silhouette)\n",
    "\n",
    "# fit KMeans to the data using the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=int((kneed+silhouette)/2))\n",
    "kmeans.fit(df_btc[['High','Low','Norm_wgt_vol']])\n",
    "pred_y = kmeans.fit_predict(df_btc[['High','Low','Norm_wgt_vol']])\n",
    "\n",
    "# Extract the cluster centers\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "df_btc['Cluster'] = pred_y\n",
    "\n",
    "# Use the cluster centers as support and resistance levels\n",
    "supports = cluster_centers[:, 0]\n",
    "resistance = cluster_centers[:, 0]\n",
    "\n",
    "#support = my_functions.list_of_tuple_to_hline(levels_2,df_btc['CloseTimestamp'])\n",
    "plot_candlestick_hline(df_btc[['CloseTimestamp','Open','High','Low','Close']], 30,20,supports, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6452e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_btc['EMA_VOL_7d'] = my_functions.rolling_kpi(df_btc,'Volume',7, 'ema', False)\n",
    "df_btc['EMA_VOL_30d'] = df_btc['Volume'].ewm(span=30, adjust=False).mean()\n",
    "df_btc['EMA_VOL_60d'] = df_btc['Volume'].ewm(span=60, adjust=False).mean()\n",
    "df_btc['EMA_VOL_100d'] = df_btc['Volume'].ewm(span=100, adjust=False).mean()\n",
    "df_btc['EMA_VOL_200d'] = df_btc['Volume'].ewm(span=200, adjust=False).mean()\n",
    "df_btc['STD_VOL_7d'] = my_functions.rolling_kpi(df_btc,'Volume',7, 'std', False)\n",
    "df_btc['STD_VOL_30d'] = my_functions.rolling_kpi(df_btc,'Volume',30, 'std', False)\n",
    "df_btc['STD_VOL_60d'] = my_functions.rolling_kpi(df_btc,'Volume',60, 'std', False)\n",
    "df_btc['STD_VOL_100d'] = my_functions.rolling_kpi(df_btc,'Volume',100, 'std', False)\n",
    "df_btc['STD_VOL_200d'] = my_functions.rolling_kpi(df_btc,'Volume',200, 'std', False)\n",
    "df_btc['PCT_Change'] = df_btc['Close'].pct_change()\n",
    "# Calculate the mean and standard deviation of the daily returns\n",
    "mean_return = df_btc['PCT_Change'].mean()\n",
    "std_return = df_btc['PCT_Change'].std()\n",
    "# Calculate the risk-free rate\n",
    "risk_free_rate = 0.01\n",
    "# Calculate the Sharpe ratio\n",
    "df_btc['Sharpe_Ratio'] = (mean_return - risk_free_rate) / std_return\n",
    "df_btc['Log_Ret'] = np.log(1 + df_btc['PCT_Change'])\n",
    "df_btc['Log_Ret_7d'] = np.log(df_btc['PCT_Change']) - np.log(df_btc['PCT_Change'].shift(7))\n",
    "df_btc['Excursion'] = df_btc['High']-df_btc['Low']\n",
    "df_btc['PCT_Excursion'] = df_btc['Excursion'].pct_change()\n",
    "df_btc['AVG_Candle_Price'] = (df_btc['Open']+df_btc['High']+df_btc['Low']+df_btc['Close'])/4\n",
    "df_btc['AVG_Candle_Price_7d'] = df_btc['AVG_Candle_Price'].ewm(span=7, adjust=False).mean()\n",
    "df_btc['AVG_Candle_Price_30d'] = df_btc['AVG_Candle_Price'].ewm(span=30, adjust=False).mean()\n",
    "df_btc['AVG_Candle_Price_60d'] = df_btc['AVG_Candle_Price'].ewm(span=60, adjust=False).mean()\n",
    "df_btc['AVG_Candle_Price_100d'] = df_btc['AVG_Candle_Price'].ewm(span=100, adjust=False).mean()\n",
    "df_btc['AVG_Candle_Price_200d'] = df_btc['AVG_Candle_Price'].ewm(span=200, adjust=False).mean()\n",
    "df_btc['STD_AVG_Candle_Price_7d'] = my_functions.rolling_kpi(df_btc,'AVG_Candle_Price',7, 'std', False)\n",
    "df_btc['STD_AVG_Candle_Price_30d'] = my_functions.rolling_kpi(df_btc,'AVG_Candle_Price',30, 'std', False)\n",
    "df_btc['STD_AVG_Candle_Price_60d'] = my_functions.rolling_kpi(df_btc,'AVG_Candle_Price',60, 'std', False)\n",
    "df_btc['STD_AVG_Candle_Price_100d'] = my_functions.rolling_kpi(df_btc,'AVG_Candle_Price',100, 'std', False)\n",
    "df_btc['STD_AVG_Candle_Price_200d'] = my_functions.rolling_kpi(df_btc,'AVG_Candle_Price',200, 'std', False)\n",
    "df_btc['PCT_Mkt_Cap'] = df_btc['Mkt_Cap']/df_btc['Mkt_Cap_tot']\n",
    "df_btc['DELTA_NTrades'] = df_btc['Number of trades']- df_btc['Number of trades'].shift()\n",
    "df_btc['EMA_Trades_7d'] = my_functions.rolling_kpi(df_btc,'Number of trades',7, 'mean', False)\n",
    "df_btc['EMA_Trades_30d'] = my_functions.rolling_kpi(df_btc,'Number of trades',30, 'mean', False)\n",
    "df_btc['EMA_Trades_60d'] = my_functions.rolling_kpi(df_btc,'Number of trades',60, 'mean', False)\n",
    "df_btc['EMA_Trades_100d'] = my_functions.rolling_kpi(df_btc,'Number of trades',100, 'mean', False)\n",
    "df_btc['EMA_Trades_200d'] = my_functions.rolling_kpi(df_btc,'Number of trades',200, 'mean', False)\n",
    "#Volume-price trend:\n",
    "df_btc['VP_trend'] = df_btc['Volume'] * df['Close'].pct_change()\n",
    "#On-balance volume\n",
    "df_btc['OBV'] = np.where(df_btc['Close'] > df_btc['Close'].shift(1), df_btc['Volume'], - df_btc['Volume'])\n",
    "df_btc['OBV'] = df_btc['OBV'].cumsum()\n",
    "#VWAP\n",
    "df_btc['VWAP'] = (df_btc['Close'] * df_btc['Volume']).cumsum() / df_btc['Volume'].cumsum()\n",
    "#Volume Rate Of Change\n",
    "df_btc['Volume_ROC'] = df_btc['Volume'].pct_change()\n",
    "#Volume-price trend divergence:\n",
    "df_btc['VP_divergence'] = df_btc['Volume'].rolling(window=10).mean() - df_btc['Close'].rolling(window=10).mean()\n",
    "#Chaikin Money Flow\n",
    "df_btc[\"chaikin_money_flow\"] = ((df_btc['Close'] - df_btc['Low']) - (df_btc['High'] - df_btc['Close'])) / (df_btc['High'] - df_btc['Low']) * df_btc['Volume']\n",
    "# Head and Shoulders\n",
    "#df_btc['hs_pattern'] = ta.pattern.head_and_shoulders(df['Close'],neck_width=1,trend='down')\n",
    "# 2 crows or 3 black crows candlestick pattern \n",
    "#df_btc['cdl2crows'] = talib.CDL2CROWS(df_btc['Open'],df_btc['High'],df_btc['Low'],df_btc['Close'])\n",
    "#df_btc['cdl3blackcrows'] = talib.CDL3BLACKCROWS(df_btc['Open'],df_btc['High'],df_btc['Low'],df_btc['Close'])\n",
    "# RSI\n",
    "df_btc['RSI'] = ta.momentum.RSIIndicator(df_btc['Close'], window=14).rsi()\n",
    "#BollingerBands\n",
    "bb = ta.volatility.BollingerBands(df_btc['Close'], window=20, window_dev=2)\n",
    "df_btc['BB_upper'] = bb.bollinger_hband()\n",
    "df_btc['BB_lower'] = bb.bollinger_lband()\n",
    "#MFI\n",
    "mfi = ta.volume.MFIIndicator(df_btc['High'], df_btc['Low'], df_btc['Close'], df_btc['Volume'], window=14)\n",
    "df_btc['MFI'] = mfi.money_flow_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b051fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_btc = labelize_output_according_criterion(df_btc, ['Open','High','Low','Close'], max_trade_length=MAX_LENGTH)\n",
    "df_btc = my_functions.labelize_output_according_criterion2(df_btc, ['Open','High','Low','Close'], max_trade_length=MAX_LENGTH)\n",
    "df_btc['signal'] = (df_btc['min_above'].lt(df_btc['min_below'])) & (df_btc['min_above'] <= MAX_LENGTH)\n",
    "df_btc['signal']=df_btc['signal'].astype(int)\n",
    "df_btc.tail(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a14281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_btc.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ac8aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_threshold = df_btc.iloc[-100]['CloseTimestamp']\n",
    "\n",
    "df_btc.drop(columns=['CloseAbovethreshold', 'HighAbovethreshold',\n",
    "       'LowAbovethreshold', 'OpenAbovethreshold', 'CloseBelowthreshold',\n",
    "       'HighBelowthreshold', 'LowBelowthreshold', 'OpenBelowthreshold', 'TP',\n",
    "       'SL', 'min_above', 'min_below'], inplace=True, errors='ignore')\n",
    "train, test = my_functions.split_train_validation(df_btc,'CloseTimestamp', dt_threshold )\n",
    "train.drop(columns=['CloseTimestamp'], inplace=True)\n",
    "test.drop(columns=['CloseTimestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb33855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get levels using the first method\n",
    "levels_1 = my_functions.detect_level_method_1(df_btc)\n",
    "# get levels using the second method\n",
    "levels_2 = my_functions.detect_level_method_2(df_btc)\n",
    "\n",
    "import mplfinance as mpf\n",
    "\n",
    "\n",
    "def plot_candlestick(df_, width, height, support,resistance):\n",
    "    \"\"\"\n",
    "    Plot candlestick chart of the dataframe\n",
    "    \"\"\"\n",
    "    dict(hlines=[support,resistance],colors=['g','r'],linestyle='-.')\n",
    "    df = df_.copy()\n",
    "    df.rename(columns={df.columns[0]:'Date'}, inplace=True)\n",
    "    df[df.columns[0]] = pd.to_datetime(df[df.columns[0]])\n",
    "    df=df.set_index(df.columns[0])\n",
    "    mpf.plot(df,type='candle', style='charles',  figsize=(width, height),alines=dict(alines=support,linestyle='-.'))\n",
    "\n",
    "\n",
    "support = my_functions.list_of_tuple_to_hline(levels_2,df_btc['CloseTimestamp'])\n",
    "\n",
    "plot_candlestick(df_btc[['CloseTimestamp','Open','High','Low','Close']], 30,20,{}, np.array(levels_2)[:,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb6db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"tscv = TimeSeriesSplit(gap=0, max_train_size=100, n_splits=60, test_size=30)\n",
    "p = {  \n",
    "    'eval_metric':'auc',\n",
    "    'tree_method':'hist',\n",
    "    'lambda':5,\n",
    "    'max_depth':3,\n",
    "    'scale_pos_weight':3,\n",
    "    'objective':'binary:logistic',\n",
    "    'eta':0.01,\n",
    "    'n_estimators':2000\n",
    "    }\n",
    "\n",
    "model = my_functions.model_selection('xgb',p)\n",
    "\n",
    "for train_index, val_index in tscv.split(train):\n",
    "    print(f'Train index:{train_index[0]}:{train_index[-1]}, Val index:{val_index[0]}:{val_index[-1]}')\n",
    "    train_features, train_labels    = my_functions.split_target_features(train.loc[train_index],cfg['TARGET_VARIABLE'])\n",
    "    val_features, val_labels        = my_functions.split_target_features(train.loc[val_index],cfg['TARGET_VARIABLE'])\n",
    "    model.fit(train_features, train_labels)\n",
    "    print(\"Test set score: {:.2f}\".format(model.score(val_features, val_labels)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63c8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# load your data\n",
    "for train_index, test_index in tscv.split(train):\n",
    "    print(f'Train index:{train_index[0]}:{train_index[-1]}, Val index:{test_index[0]}:{test_index[-1]}')\n",
    "    train_features, train_labels    = my_functions.split_target_features(train.loc[train_index],cfg['TARGET_VARIABLE'])\n",
    "    val_features, val_labels        = my_functions.split_target_features(train.loc[test_index],cfg['TARGET_VARIABLE'])\n",
    "    X_train, X_test = train_features, val_features\n",
    "    y_train, y_test = train_labels, val_labels\n",
    "\n",
    "    # create the ensemble model\n",
    "    clf2 = DecisionTreeClassifier()\n",
    "    clf3 = xgb.XGBClassifier()\n",
    "\n",
    "    ensemble_clf = VotingClassifier(estimators=[('dt', clf2), ('svc', clf3)], voting='hard')\n",
    "\n",
    "    # train the ensemble model\n",
    "    ensemble_clf.fit(X_train.fillna(-1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b136a57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_clf.score(X_test.fillna(-1),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09802db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e013af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.layers import Dropout, Activation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "\n",
    "X_train, y_train = my_functions.split_target_features(train[:-100],cfg['TARGET_VARIABLE'])\n",
    "X_val, y_val = my_functions.split_target_features(train[-100:],cfg['TARGET_VARIABLE'])\n",
    "#\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#X_train = scaler.fit_transform(X_train.fillna(-1))\n",
    "#X_val = scaler.transform(X_val.fillna(-1))\n",
    "\n",
    "# Create the model\n",
    "X_train = X_train.fillna(-1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(LSTM(100, return_sequences=False))\n",
    "model.add(Dense(50))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train, y_train, batch_size= 5, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0910bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = my_functions.split_target_features(test,cfg['TARGET_VARIABLE'])\n",
    "#X_test_scaled = scaler.transform(X_test.fillna(-1))\n",
    "y_pred = model.predict(X_test.fillna(-1))\n",
    "\n",
    "# Print the predictions\n",
    "#print(y_pred)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Calculate the AUC\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Print the AUC\n",
    "print(\"AUC: \", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aae8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fae67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def cm_analysis(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    title: str,\n",
    "    filename: str, \n",
    "    labels: List[str], \n",
    "    ymap=None, \n",
    "    figsize=(10,10)\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate matrix plot of confusion matrix with pretty annotations.\n",
    "    The plot image is saved to disk.\n",
    "\n",
    "    args: \n",
    "      y_true:    true label of the data, with shape (nsamples,)\n",
    "      y_pred:    prediction of the data, with shape (nsamples,)\n",
    "      title:     plot name\n",
    "      filename:  filename of figure file to save\n",
    "      labels:    string array, name the order of class labels in the confusion matrix.\n",
    "                 use `clf.classes_` if using scikit-learn models.\n",
    "                 with shape (nclass,).\n",
    "      ymap:      dict: any -> string, length == nclass.\n",
    "                 if not None, map the labels & ys to more understandable strings.\n",
    "                 Caution: original y_true, y_pred and labels must align.\n",
    "      figsize:   the size of the figure plotted.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    if ymap is not None:\n",
    "        y_pred = [ymap[yi] for yi in y_pred]\n",
    "        y_true = [ymap[yi] for yi in y_true]\n",
    "        labels = [ymap[yi] for yi in labels]\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.2f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.2f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    cm_perc = pd.DataFrame(cm_perc, index=labels, columns=labels)\n",
    "    cm_perc.index.name = 'Actual'\n",
    "    cm_perc.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(cm_perc, annot=annot, fmt='', linewidths=1, ax=ax)\n",
    "    plt.title('\\n'+title+'\\n', fontsize=14)\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "y_hat = [1 if i>.5 else 0 for i in y_pred]\n",
    "cm_analysis(y_test, y_hat,title='asdasda',filename='cm.png',labels=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baa4e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection(\n",
    "    model_type: str,\n",
    "    parameter: Dict\n",
    ") -> xgb.XGBClassifier:\n",
    "    \"\"\"\n",
    "    Instantiate a tree based model\n",
    "    \n",
    "     Args:\n",
    "        model_type: xgb/lgb currently supported\n",
    "        parameter:  default parameters \n",
    "    \n",
    "    Returns:\n",
    "        Union[xgb.XGBClassifier, LGBMClassifier]: model instantiated\n",
    "    \"\"\"\n",
    "\n",
    "    dict_model = {\n",
    "            'xgb': xgb.XGBClassifier(n_jobs=-1, random_state=42),\n",
    "            'rf': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
    "        }\n",
    "\n",
    "    return dict_model[model_type].set_params(**parameter)\n",
    "\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=15, test_size=100)\n",
    "p = {  \n",
    "    'eval_metric':'auc',\n",
    "    'tree_method':'hist',\n",
    "    'lambda':5,\n",
    "    'gamma':2,\n",
    "    'max_depth':7,\n",
    "    'scale_pos_weight':2,\n",
    "    'objective':'binary:logistic',\n",
    "    #'subsample': .95,\n",
    "    'colsample_bytree': .9,\n",
    "    'min_child_weight':1,\n",
    "    'eta':0.05,\n",
    "    'n_estimators':2000\n",
    "}\n",
    "\n",
    "model = model_selection('xgb',p)\n",
    "\n",
    "\n",
    "for train_index, test_index in tscv.split(train):\n",
    "    print(f'Train index:{train_index[0]}:{train_index[-1]}, Val index:{test_index[0]}:{test_index[-1]}')\n",
    "    train_features, train_labels    = my_functions.split_target_features(train.loc[train_index],cfg['TARGET_VARIABLE'])\n",
    "    val_features, val_labels        = my_functions.split_target_features(train.loc[test_index],cfg['TARGET_VARIABLE'])\n",
    "    X_train, X_test = train_features, val_features\n",
    "    y_train, y_test = train_labels, val_labels\n",
    "\n",
    "    model.fit(X_train, y_train, eval_set = [(X_train, y_train), (X_test, y_test)], verbose=500)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29dbeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict_proba(train[train.columns[:-1]])[:,1]\n",
    "y = train['signal']\n",
    "df_test= pd.DataFrame({'y':y,'y_hat':y_hat})\n",
    "df_test = pd.merge(train, df_test, left_index=True, right_index=True)\n",
    "df_test = df_test[df_test.columns[1:]]\n",
    "df_test.sort_values(by=['y_hat'], ascending=False, inplace=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3503dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(train[train.columns[:-1]])\n",
    "shap.summary_plot(shap_values, train[train.columns[:-1]], plot_size=[30,30], max_display=len(train[train.columns[:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca23cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#train.drop(columns=['Open',\n",
    "# 'EMA_Trades_100d',\n",
    "# 'EMA_Trades_7d',\n",
    "# 'AVG_Candle_Price',\n",
    "# 'STD_VOL_100d',\n",
    "# 'STD_VOL_30d',\n",
    "# 'EMA_VOL_200d',\n",
    "# 'Log_Ret',\n",
    "# 'High',\n",
    "# 'Low'], inplace=True)\n",
    "tscv = TimeSeriesSplit(gap=0, max_train_size=100, n_splits=36, test_size=30)\n",
    "p = {  \n",
    "    'eval_metric' : ['auc'],\n",
    "    'tree_method': ['hist'],\n",
    "    'lambda':[5],\n",
    "    'max_depth':[3],\n",
    "    'scale_pos_weight':[3],\n",
    "    'objective':['binary:logistic'],\n",
    "    'eta':[0.01],\n",
    "    'early_stopping_rounds':[100],\n",
    "    'n_estimators': [1000, 2000]\n",
    "    }\n",
    "\n",
    "\n",
    "custom_cv = []\n",
    "\n",
    "for i in tscv.split(train):\n",
    "    custom_cv.append((i[0],i[1]))\n",
    "\n",
    "model = my_functions.model_selection('xgb',{'early_stopping_rounds':100})\n",
    "#model.set_params(n_estimators=2000)\n",
    "gridSearch = GridSearchCV(estimator=model, param_grid=p, cv=custom_cv)\n",
    "train_features, train_labels    = my_functions.split_target_features(train,cfg['TARGET_VARIABLE'])\n",
    "gridSearch.fit(train_features, # shoud have shape of (n_samples, n_features) \n",
    "                train_labels) #this should be an array with shape (n_samples,)\n",
    "print(gridSearch.best_score_, gridSearch.best_params_)\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(tscv.split(train)):\n",
    "    print(f'iteration: {i}, Train index:{train_index[0]}:{train_index[-1]}, Val index:{val_index[0]}:{val_index[-1]}')\n",
    "    train_features, train_labels    = my_functions.split_target_features(train.loc[train_index],cfg['TARGET_VARIABLE'])\n",
    "    val_features, val_labels        = my_functions.split_target_features(train.loc[val_index],cfg['TARGET_VARIABLE'])\n",
    "    model_ = my_functions.dummy_train(\n",
    "                    train_features, \n",
    "                    train_labels, \n",
    "                    val_features, \n",
    "                    val_labels, \n",
    "                    {  \n",
    "                    'eval_metric' : 'auc',\n",
    "                    'tree_method': 'hist',\n",
    "                    'lambda':5,\n",
    "                    'max_depth':3,\n",
    "                    'scale_pos_weight':10,\n",
    "                    'objective':'binary:logistic',\n",
    "                    'eta':0.005,\n",
    "                    }, \n",
    "                    2000,\n",
    "                    'xgb',\n",
    "                    verbosity=None\n",
    "            )\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb3045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,20)) \n",
    "plot_importance(model, ax=ax, importance_type='gain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba0753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_functions.plt_correlation(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf7d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''test.drop(columns=['Open',\n",
    " 'EMA_Trades_100d',\n",
    " 'EMA_Trades_7d',\n",
    " 'AVG_Candle_Price',\n",
    " 'STD_VOL_100d',\n",
    " 'STD_VOL_30d',\n",
    " 'EMA_VOL_200d',\n",
    " 'Log_Ret',\n",
    " 'High',\n",
    " 'Low'], inplace=True)'''\n",
    "y_hat = clf.predict_proba(test.drop(columns='signal'))[:,1]\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf827c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr_test, tpr_test, _ = sklearn.metrics.roc_curve(test['signal'], y_hat)\n",
    "auc_test = sklearn.metrics.auc(fpr_test, tpr_test)\n",
    "print(auc_test)\n",
    "y_true = test['signal'].to_list()\n",
    "y_probas = y_hat.tolist()\n",
    "skplt.metrics.plot_roc_curve(test['signal'],clf.predict_proba(test.drop(columns='signal').fillna(-1)), title=f\"ROC curve, AUC=test: {auc_test:.4f}\", curves=('macro',1), figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e69f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fa92fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "dd184c5f03045ecc89fe53694d3e4314236225fa9c7db50520ba8ca1ba4d33a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
