{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8736073a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<coinmetrics.api_client.CoinMetricsClient object at 0x000001FA6227B640>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import requests\n",
    "import json\n",
    "import sys, os\n",
    "import itertools\n",
    "import time\n",
    "import numpy as np\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "from binance.spot import Spot\n",
    "from twelvedata import TDClient\n",
    "from sklearn import metrics\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "import itertools\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "from typing import Union, Dict, List\n",
    "import talib\n",
    "import ta\n",
    "import shap\n",
    "from coinmetrics.api_client import CoinMetricsClient\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"..\"))\n",
    "import fin_utilities \n",
    "import sklearn\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "cfg = fin_utilities.__cfg_reading(\"pred\")\n",
    "import my_functions\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Bidirectional, Dense,BatchNormalization\n",
    "from keras.layers import Dropout, Activation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "# Set the default color cycle\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "\n",
    "\n",
    "#td = TDClient(apikey=cfg['TWELVEDATA']['API'])  \n",
    "\n",
    "PROJECT_DIR = eval(cfg['PROJECT_PATH'])\n",
    "DATA_PATH   = PROJECT_DIR / cfg['DATA_FOLDER']\n",
    "SOURCE      = cfg['API_DATA_SOURCE']\n",
    "URL         = cfg[SOURCE]['API_URL_HIST_DATA']\n",
    "COLUMNS     = cfg[SOURCE]['COLUMN_NAMES']\n",
    "PARAMS      = cfg[SOURCE]['REQ_PARAMS']\n",
    "SYMBOL      = cfg['SYMBOL']\n",
    "STABLECOIN  = cfg['STABLECOIN']\n",
    "MAX_LENGTH  = cfg['MAX_TRADE_DURATION']\n",
    "XGB_PARAM   = cfg['xgb']\n",
    "TEST_SIZE   = 100\n",
    "client = CoinMetricsClient()\n",
    "print(client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_values(dict_list, key):\n",
    "    \"\"\"\n",
    "    Extracts the values associated with a given key from a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        dict_list (list): A list of dictionaries.\n",
    "        key (str): The key to extract the value from.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of values associated with the key.\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    for dictionary in dict_list:\n",
    "        if key in dictionary:\n",
    "            values.append(dictionary[key])\n",
    "    return values\n",
    "\n",
    "\n",
    "def create_dict(list1, list2):\n",
    "    \"\"\"\n",
    "    Given two lists, create a dictionary with keys from list1 and values from list2\n",
    "\n",
    "    Args:\n",
    "        list1 (list): The list of keys for the dictionary.\n",
    "        list2 (list): The list of values for the dictionary.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with keys from list1 and values from list2, intercepted with available metrics\n",
    "    \"\"\"\n",
    "    result_dict = {}\n",
    "    for key in list1:\n",
    "        result_dict[key] = list(set(extract_values(client.catalog_assets(key)[0]['metrics'], 'metric')) & set(list2))\n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688c321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYMBOL = ['btc']\n",
    "\n",
    "frequency  = \"1d\"\n",
    "asset      = SYMBOL\n",
    "metrics_a = [\n",
    "    #'CapAct1yrUSD',\n",
    "    'CapMVRVCur',\n",
    "    'CapMVRVFF',\n",
    "    #'CapMrktCurUSD',\n",
    "    #'CapMrktEstUSD',\n",
    "    #'CapMrktFFUSD',\n",
    "    'CapRealUSD',\n",
    "    'AdrActCnt',\n",
    "    'FeeTotUSD',\n",
    "    'FeeMeanUSD',\n",
    "    'DiffLast',\n",
    "    'DiffMean',\n",
    "    'NDF',\n",
    "    'TxCnt',\n",
    "    'TxCntSec',\n",
    "    #'TxTfrCnt'\n",
    "]\n",
    "\n",
    "\n",
    "d = create_dict(SYMBOL, metrics_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8780c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "for asset, metrics_asset in d.items():\n",
    "    df_list.append(client.get_asset_metrics(\n",
    "                                    assets    = asset,\n",
    "                                    metrics   = metrics_asset,\n",
    "                                    frequency = frequency,\n",
    "                                    start_time= PARAMS['startTime'],\n",
    "                                    end_time  = PARAMS['endTime'],\n",
    "                                    page_size = 10000\n",
    "                                ).to_dataframe()\n",
    "    )\n",
    "\n",
    "df_mcap = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "#droppo tutte le colonne \"status\" inutili\n",
    "df_mcap.drop(columns=[col for col in df_mcap.columns if 'status' in col], inplace=True)\n",
    "\n",
    "#replace di tutti i campi None stringa a -1\n",
    "for col in df_mcap.columns[2:]:\n",
    "    if len(df_mcap[df_mcap[col]=='None'])!=0:\n",
    "        df_mcap[col]=df_mcap[col].replace('None','-1')\n",
    "    else:\n",
    "        df_mcap[col]=df_mcap[col].fillna(-1)\n",
    "\n",
    "df_mcap = df_mcap.astype('float64', errors='ignore')\n",
    "df_mcap = my_functions.round_float_cols(df_mcap)\n",
    "\n",
    "#df_mcap = df_mcap[df_mcap['AdrActCnt']!=0].reset_index(drop=True)\n",
    "#df_mcap = df_mcap[df_mcap['AdrActCnt']!=None].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0d2b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_functions.plt_correlation(df_mcap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ed256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT DI TUTTI I DATI DA TUTTI GLI EXCHANGE\n",
    "from typing import Optional\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "stream_handler = logging.StreamHandler()\n",
    "level = logging.getLevelName(\"INFO\")\n",
    "stream_handler.level = level\n",
    "formatter = logging.Formatter(\n",
    "    datefmt=\"[%Y-%m-%d %H:%M:%S]\", fmt=\"%(asctime)-15s %(levelname)s %(message)s\"\n",
    ")\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.level = level\n",
    "\n",
    "# use it if you want to get specific exchanges or leave it empty if you want to get all exchanges data\n",
    "EXCHANGES_TO_EXPORT = {}\n",
    "\n",
    "# use it if you want to get specific markets or leave it empty if you want to get all markets\n",
    "# example of market name to be used in this filter: \"binance-BTCUSDT-future\",\n",
    "# note that if you specified exchanges filter, it will act as selecting intersection with the markets to export\n",
    "# not as union.\n",
    "MARKETS_TO_EXPORT = {}\n",
    "\n",
    "# example values: \"spot\", \"future\", \"option\"\n",
    "# you can use all 3 if you want or just a subset\n",
    "MARKET_TYPES_TO_COLLECT = {\n",
    "    \"spot\"   \n",
    "}\n",
    "\n",
    "# leave it empty to catch all\n",
    "BASE_MARKETS = {\n",
    "    \"btc\",\n",
    "}\n",
    "\n",
    "# leave it empty to catch all\n",
    "QUOTE_MARKETS = {\n",
    "    \"usdt\",\n",
    "}\n",
    "\n",
    "# 1m, 5m, 10m, 15m, 30m, 1h, 4h, 1d\n",
    "FREQUENCY = \"1d\"\n",
    "\n",
    "DST_ROOT = \"./data\"\n",
    "EXPORT_START_DATE = \"2009-01-01\"\n",
    "EXPORT_END_DATE: Optional[str] = None\n",
    "# path to local file that is used to not reexport data if it was already exported\n",
    "PROCESSED_DAYS_REGISTRY_FILE_PATH = \"candles_processed_days_registry.txt\"\n",
    "\n",
    "client = CoinMetricsClient()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    export_start_time = datetime.datetime.now()\n",
    "    try:\n",
    "        my_functions.export_data(\n",
    "                client,\n",
    "                PROCESSED_DAYS_REGISTRY_FILE_PATH, \n",
    "                EXCHANGES_TO_EXPORT,\n",
    "                MARKETS_TO_EXPORT,\n",
    "                MARKET_TYPES_TO_COLLECT,\n",
    "                BASE_MARKETS,\n",
    "                QUOTE_MARKETS,\n",
    "                EXPORT_START_DATE,\n",
    "                str(datetime.datetime.now().date()), \n",
    "                DST_ROOT, \n",
    "                FREQUENCY \n",
    "        )\n",
    "    finally:\n",
    "        print(\"export took: %s\", datetime.datetime.now() - export_start_time)\n",
    "\n",
    "\n",
    "mkts = my_functions.get_markets_to_process(client,EXCHANGES_TO_EXPORT, MARKETS_TO_EXPORT, MARKET_TYPES_TO_COLLECT, BASE_MARKETS, QUOTE_MARKETS)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for mkt in mkts:\n",
    "    print(mkt) \n",
    "    \n",
    "    market_data_root = \"/\".join(                                                                                                                                    \n",
    "                (                                                                                                                                                        \n",
    "                    DST_ROOT.rstrip(\"/\"),                                                                                                                                \n",
    "                    mkt[\"market\"].split(\"-\")[0],                                                                                                                         \n",
    "                    my_functions.get_instrument_root(mkt),                                                                                                                            \n",
    "                ) \n",
    "    )      \n",
    "    print(f'manng {my_functions.get_instrument_root(mkt)}')\n",
    "\n",
    "    df = pd.concat([df,\n",
    "                    my_functions.read_data_for_a_market(\n",
    "                                market_data_root,                                                                                                                                                               \n",
    "                                FREQUENCY                                         \n",
    "                            )\n",
    "                    ],\n",
    "                     ignore_index=True)       \n",
    "\n",
    "#max volums for each day between all the exchanges\n",
    "df = df[df['volume'] == df.groupby('time')['volume'].transform('max')].sort_values(by='time').reset_index(drop=True)\n",
    "df['time'] = pd.to_datetime(df['time']).dt.date\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df['market'] = df['market'].str.split('-',2, expand=True)[1]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e99df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dxy = pd.read_csv('data/DXY.csv')\n",
    "dxy = dxy[['Date','Adj Close']]\n",
    "dxy['Date'] = pd.to_datetime(dxy['Date'])\n",
    "#inserisco date mancanti\n",
    "dxy = pd.merge(pd.Series(pd.date_range(dxy['Date'].min(),dxy['Date'].max(),freq='d'), name='Date'), dxy, on='Date', how='left').reset_index(drop=True)\n",
    "dxy['Dxy_Close'] = np.round(dxy['Adj Close'].ffill(),2)\n",
    "dxy.drop(columns='Adj Close', inplace=True)\n",
    "df = pd.merge(df,dxy,left_on='time',right_on='Date',how='left')\n",
    "df.drop(columns=['Date'],inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_ohlc = {'Open':'price_open','High':'price_high','Low':'price_low','Close':'price_close', 'Volume': 'volume', 'Number of trades': 'candle_trades_count'}\n",
    "lags_relevant = [13,30,99,200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35efa46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "df['AVG_Candle_Price'] = (df[map_ohlc['Open']]+df[map_ohlc['High']]+df[map_ohlc['Low']]+df[map_ohlc['Close']])/4\n",
    "for lag in lags_relevant:\n",
    "    df['Close_lag_'+str(lag)+'d'] = df[map_ohlc['Close']].shift(lag)\n",
    "    df['SMA_Close_'+str(lag)+'d'] = my_functions.rolling_kpi(df,map_ohlc['Close'],lag,'mean', False)\n",
    "    df[map_ohlc['Number of trades']+'_'+str(lag)+'d'] = my_functions.rolling_kpi(df,map_ohlc['Number of trades'], lag, 'mean', False)\n",
    "    df['STD_Close_'+str(lag)+'d'] = my_functions.rolling_kpi(df,map_ohlc['Close'],lag,'std', False)\n",
    "    df['VAR_Close_lag_'+str(lag)+'d'] = df['STD_Close_'+str(lag)+'d']**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8211563",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = my_functions.labelize_output_according_criterion2(df, map_ohlc, max_trade_length=MAX_LENGTH)\n",
    "df['signal'] = (df['min_above'].lt(df['min_below'])) & (df['min_above'] <= MAX_LENGTH)\n",
    "df['signal']=df['signal'].astype(int)\n",
    "df.drop(columns=['CloseAbovethreshold', 'HighAbovethreshold',\n",
    "       'LowAbovethreshold', 'OpenAbovethreshold', 'CloseBelowthreshold',\n",
    "       'HighBelowthreshold', 'LowBelowthreshold', 'OpenBelowthreshold', 'TP',\n",
    "       'SL', 'min_above', 'min_below'], inplace=True, errors='ignore')\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576df96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "def yeojohntrans(df, feature):   # function to apply transformer and check the distribution with histogram and kdeplot\n",
    "    \n",
    "    yeojohnTr = PowerTransformer(standardize=True)   # not using method attribute as yeo-johnson is the default\n",
    "\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(\"Distribution before Transformation\", fontsize=15)\n",
    "    sns.histplot(df[feature], kde=True, color=\"red\")\n",
    "    plt.subplot(1,2,2)\n",
    "    \n",
    "    df_yeojohn = pd.DataFrame(yeojohnTr.fit_transform(df[feature].values.reshape(-1,1)))\n",
    "    plt.title(\"Distribution after Transformation\", fontsize=15)\n",
    "    sns.histplot(df_yeojohn,bins=20, kde=True , legend=False)\n",
    "    plt.xlabel(feature)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Skewness was {round(df[feature],2)} before & is {round(df_yeojohn.skew()[0],2)} after Yeo-johnson transformation.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddca27a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df.copy()\n",
    "\n",
    "features = [col for col in df_analysis.columns if col not in ['market','time','signal']]\n",
    "fig, axs = plt.subplots(nrows=int(len(features)/3), ncols=3, figsize=(18,5*int(len(features)/3)))\n",
    "for i,feature in enumerate(features):\n",
    "    df_analysis[feature] = df_analysis[feature] - df_analysis[feature].shift()\n",
    "    single_feature = df_analysis[feature][1:]\n",
    "    sns.histplot(data=single_feature,kde=True, stat='density', ax=axs[int(i/3)][i%3])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d89668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_btc = df.copy()\n",
    "df_btc.dropna(inplace=True)\n",
    "df_btc.drop(columns='market', inplace=True)\n",
    "df_btc = df_btc.reset_index(drop=True)\n",
    "df_btc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e1d0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654504dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_btc.drop(columns=['price_close','price_high','price_low','candle_usd_volume', 'vwap','volume','candle_trades_count','Dxy_Close'], inplace=True)\n",
    "df_btc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136613b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "features = [col for col in df_btc.columns if col not in ['time','signal']]\n",
    "for feature in features:\n",
    "     df_btc[feature] = df_btc[feature] - df_btc[feature].shift()\n",
    "     df_btc[feature] = np.sqrt(np.absolute(df_btc[feature] - df_btc[feature].shift().fillna(0)))*np.sign(df_btc[feature] - df_btc[feature].shift().fillna(0))\n",
    "\n",
    "df_btc = df_btc.iloc[1:]\n",
    "\n",
    "df_btc = my_functions.round_float_cols(df_btc)\n",
    "df_btc = df_btc.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "scaler.fit(df_btc[features][:-TEST_SIZE])\n",
    "\n",
    "X_train = df_btc[features][:-TEST_SIZE]\n",
    "X_test = df_btc[features][-TEST_SIZE:]\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f075be44",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.array(df_btc['signal'][:-TEST_SIZE])\n",
    "Y_test = np.array(df_btc['signal'][-TEST_SIZE:])\n",
    "\n",
    "#X_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\n",
    "#X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f7d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {  \n",
    "    'eval_metric':'auc',\n",
    "    'tree_method':'hist',\n",
    "    'lambda':.01,\n",
    "    'max_depth':7,\n",
    "    'scale_pos_weight':5,\n",
    "    'objective':'binary:logistic',\n",
    "    'colsample_bytree':.9,\n",
    "    'eta':0.1,\n",
    "    'n_estimators':3000,\n",
    "    'early_stopping_rounds':200\n",
    "    }\n",
    "\n",
    "model = my_functions.model_selection('xgb',p)\n",
    "\n",
    "print(model.get_params())\n",
    "\n",
    "model.fit(X_train[:-100], Y_train[:-100], eval_set = [(X_train[:-100], Y_train[:-100],), (X_train[-100:], Y_train[-100:],)], verbose=0)\n",
    "predictions = model.predict_proba(X_train[-100:])[:,1]\n",
    "fpr_test, tpr_test, _ = sklearn.metrics.roc_curve(Y_train[-100:], predictions)\n",
    "print(\"Test set score: {:.5f}\".format(sklearn.metrics.auc(fpr_test, tpr_test)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7525e90c",
   "metadata": {},
   "source": [
    "BUILD LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef2891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.optimizers import Adagrad\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.experimental.SGD(learning_rate=.2, weight_decay=0)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, activation='relu', input_shape=(X_train.shape[1],1), kernel_initializer=initializers.RandomNormal(stddev=0.1)))\n",
    "model.add(LSTM(50, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "            optimizer=opt,\n",
    "            loss=['binary_crossentropy'], \n",
    "            metrics=['AUC','accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=50, epochs=5000, validation_split=0.1, shuffle=True)\n",
    "\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy    \n",
    "plt.plot(history.history['auc'])\n",
    "plt.title('model AUC')\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65bed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pr = model.predict(X_train)\n",
    "y_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f92df47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(LSTM(units=1024, kernel_initializer='random_normal', return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model2.add(Dropout(0.4))\n",
    "model2.add(LSTM(64))\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dropout(0.4))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model2.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model2.summary()\n",
    "\n",
    "# Fit the model to the data\n",
    "history = model2.fit(X_train, Y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8d5ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(LSTM(1025, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model3.add(LSTM(256, return_sequences=True))\n",
    "model3.add(LSTM(128, return_sequences=False))   \n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a2b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=True, input_shape=(1, X_train.shape[1])))\n",
    "model.add(LSTM(100, return_sequences=False))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a9886",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c6d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assume your DataFrame is called df\n",
    "\n",
    "# Step 1: Convert DataFrame to a numpy array\n",
    "data = X_train\n",
    "print(np.shape(X_train))\n",
    "# Step 2: Define the window size, i.e., the number of time steps to look back\n",
    "window_size = 30\n",
    "\n",
    "# Step 3: Create empty lists to hold the inputs and outputs\n",
    "X_train = []\n",
    "Y_train = np.array(Y_train[window_size:])\n",
    "\n",
    "# Step 4: Loop through the data and create the inputs and outputs\n",
    "for i in range(window_size, len(data)):\n",
    "    X_train.append(data[i-window_size:i])\n",
    "#    y.append(data[len(features)-1][0])  # use the close price as the output variable\n",
    "\n",
    "# Step 5: Convert the lists to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "# Step 6: Reshape the input data into a 3D tensor\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], len(features)))\n",
    "#X_train = np.transpose(X_train, (0, 2,1))\n",
    "\n",
    "\n",
    "#X_test = data[-(TEST_SIZE+window_size): , : ]\n",
    "X_test = []\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "# Step 4: Loop through the data and create the inputs and outputs\n",
    "for i in range(len(data)-TEST_SIZE, len(data)):\n",
    "    X_test.append(data[i-window_size:i])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "#X_test = np.transpose(X_test, (0, 2,1))\n",
    "\n",
    "print(np.shape(X_train), np.shape(Y_train))\n",
    "print(np.shape(X_test), np.shape(Y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca485a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(X_train[0][0]))\n",
    "np.shape(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb9b6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#model = Sequential()\n",
    "#model.add(LSTM(50, return_sequences=True, kernel_regularizer=regularizers.l2(0.01),  kernel_initializer=initializers.RandomNormal(stddev=0.01), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "#model.add(LSTM(200, return_sequences=False))\n",
    "#model.add(Dense(25))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt = tf.keras.optimizers.experimental.SGD(learning_rate=.1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(20, return_sequences=True, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=0.01), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(50,return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(50,return_sequences=False, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=opt, loss=['binary_crossentropy'], metrics=['AUC','accuracy'])\n",
    "history = model.fit(X_train, Y_train, batch_size=10, epochs=100, use_multiprocessing=True)\n",
    "\n",
    "\n",
    "\"\"\"import tensorflow as tf\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(\n",
    "    LSTM(500, input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    )\n",
    ")\n",
    "model.add(LSTM(200, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.experimental.Adam(\n",
    "    learning_rate=0.02), loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size= 30, epochs=50, shuffle=False)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb503fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy    \n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['auc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['accuracy', 'AUC'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904be153",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pr = model.predict(X_train)\n",
    "y_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407a1d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions = predictions[:,0]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394a50ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (predictions-np.min(predictions))/(np.max(predictions)-np.min(predictions))\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba0753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_functions.plt_correlation(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d33016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "best_precision = 0\n",
    "thresh = 0\n",
    "for i in np.arange(0.01, 1, 0.01):\n",
    "    y_pred = [1 if el > i else 0 for el in predictions]\n",
    "    #precision = sklearn.metrics.accuracy_score(Y_test, y_pred) # average='weighted', beta=.5)\n",
    "    precision = sklearn.metrics.fbeta_score(Y_test, y_pred, average='weighted', beta=.5)\n",
    "    if best_precision < precision:\n",
    "        thresh = i \n",
    "        best_precision = precision\n",
    "\n",
    "print(best_precision)\n",
    "print(thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a462fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [1 if el>thresh else 0 for el in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5357e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def cm_analysis(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    title: str,\n",
    "    filename: str, \n",
    "    labels: List[str], \n",
    "    ymap=None, \n",
    "    figsize=(10,10)\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate matrix plot of confusion matrix with pretty annotations.\n",
    "    The plot image is saved to disk.\n",
    "\n",
    "    args: \n",
    "      y_true:    true label of the data, with shape (nsamples,)\n",
    "      y_pred:    prediction of the data, with shape (nsamples,)\n",
    "      title:     plot name\n",
    "      filename:  filename of figure file to save\n",
    "      labels:    string array, name the order of class labels in the confusion matrix.\n",
    "                 use `clf.classes_` if using scikit-learn models.\n",
    "                 with shape (nclass,).\n",
    "      ymap:      dict: any -> string, length == nclass.\n",
    "                 if not None, map the labels & ys to more understandable strings.\n",
    "                 Caution: original y_true, y_pred and labels must align.\n",
    "      figsize:   the size of the figure plotted.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    if ymap is not None:\n",
    "        y_pred = [ymap[yi] for yi in y_pred]\n",
    "        y_true = [ymap[yi] for yi in y_true]\n",
    "        labels = [ymap[yi] for yi in labels]\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.2f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.2f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    cm_perc = pd.DataFrame(cm_perc, index=labels, columns=labels)\n",
    "    cm_perc.index.name = 'Actual'\n",
    "    cm_perc.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(cm_perc, annot=annot, fmt='', linewidths=1, ax=ax)\n",
    "    plt.title('\\n'+title+'\\n', fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "cm_analysis(Y_test, y_pred,title='asdasda',filename='cm.png',labels=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf827c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_test, tpr_test, _ = sklearn.metrics.roc_curve(Y_test, predictions)\n",
    "auc_test = sklearn.metrics.auc(fpr_test, tpr_test)\n",
    "print(auc_test)\n",
    "#skplt.metrics.plot_roc(df_test['signal'],y_probas, title=f\"ROC curve, AUC=test: {auc_test:.4f}\", classes_to_plot=[1], figsize=(10,10))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56fc0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e9f850",
   "metadata": {},
   "outputs": [],
   "source": [
    "close_prices = df['price_close']\n",
    "values = close_prices.values\n",
    "training_data_len = int(len(values)* 0.8)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_data = scaler.fit_transform(values.reshape(-1,1))\n",
    "train_data = scaled_data[0: training_data_len, :]\n",
    "\n",
    "\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(60, len(train_data)):\n",
    "    x_train.append(train_data[i-60:i, 0])\n",
    "    y_train.append(train_data[i, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc4db1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(60, len(train_data)):\n",
    "    x_train.append(train_data[i-60:i, 0])\n",
    "    y_train.append(train_data[i, 0])\n",
    "\n",
    "print(np.shape(x_train), np.shape(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d09b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58768af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(x_train), np.shape(y_train))\n",
    "\n",
    "test_data = scaled_data[training_data_len-60: , : ]\n",
    "x_test = []\n",
    "y_test = values[training_data_len:]\n",
    "\n",
    "for i in range(60, len(test_data)):\n",
    "  x_test.append(test_data[i-60:i, 0])\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "print(np.shape(x_test), np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
    "model.add(LSTM(100, return_sequences=False))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6383658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(x_train, y_train, batch_size= 1, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe861b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "rmse = np.sqrt(np.mean(predictions - y_test)**2)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea031f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.filter(['price_close'])\n",
    "train = data[:training_data_len]\n",
    "validation = data[training_data_len:]\n",
    "validation['Predictions'] = predictions\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Model')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price USD ($)')\n",
    "plt.plot(train)\n",
    "plt.plot(validation[['price_close', 'Predictions']])\n",
    "plt.legend(['Train', 'Val', 'Predictions'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c28a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assume your DataFrame is called df\n",
    "\n",
    "# Step 1: Convert DataFrame to a numpy array\n",
    "data = np.array(df[features])\n",
    "\n",
    "# Step 2: Define the window size, i.e., the number of time steps to look back\n",
    "window_size = 50\n",
    "\n",
    "# Step 3: Create empty lists to hold the inputs and outputs\n",
    "X = []\n",
    "y = df['signal'][window_size:]\n",
    "\n",
    "# Step 4: Loop through the data and create the inputs and outputs\n",
    "for i in range(window_size, len(data)):\n",
    "    X.append(data[i-window_size:i])\n",
    "#    y.append(data[len(features)-1][0])  # use the close price as the output variable\n",
    "\n",
    "# Step 5: Convert the lists to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Step 6: Reshape the input data into a 3D tensor\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], len(features)))\n",
    "X2 = np.transpose(X, (0, 2,1))\n",
    "print(np.shape(X), np.shape(y))\n",
    "print(np.shape(X2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83f6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8356f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Flatten, Dropout\n",
    "from keras.optimizers import Adagrad\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, return_sequences=True, activation='relu', input_shape=(X2.shape[1],X2.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='rmsprop',    \n",
    "              metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ebe689",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X2, y, batch_size= 1, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f498ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c4c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259b70fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "dd184c5f03045ecc89fe53694d3e4314236225fa9c7db50520ba8ca1ba4d33a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
